# ğŸª¦ google-research/bert

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                     â”‚
â”‚         google-research/bert           â”‚
â”‚                                     â”‚
â”‚         2018-10-25 â€” 2024-07-23      â”‚
â”‚                                     â”‚
â”‚         â­ 39835 stars              â”‚
â”‚         ğŸ“ Python               â”‚
â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Epitaph

You taught the machines to attend to the words,
Pre-trained weights on shoulders of giants.
Though TensorFlow gathers dust and the epoch ends,
Thirty-nine thousand stars still shine in the silence.
The context is masked, and the attention is gone.
Rest in /dev/null, Old State Farm model.

---

**Description:** TensorFlow code and pre-trained models for BERT

**Topics:** google, natural-language-processing, natural-language-understanding, nlp, tensorflow

**Archived:** Yes

**Repository:** [google-research/bert](https://github.com/google-research/bert)

---

*Epitaph composed by Claude Sonnet*

*Tombstone #55 in the graveyard*

*May your code live on in forks.*
